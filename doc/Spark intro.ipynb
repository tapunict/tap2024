{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Unified engine for large-scale data analytics\n",
    "Apache Spark™ is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters \n",
    "![](https://spark.apache.org/images/spark-logo-trademark.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Apache Spark project's History\n",
    "[A Gentle Introduction to Apache Spark on Databricks](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/346304/2168141618055043/484361/latest.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Spark was originally written by the founders of Databricks during their time at UC Berkeley. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://i.imgflip.com/8o8ag0.jpg)\n",
    "[Nicsmeme](https://imgflip.com/i/8o8ag0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Spark project started in 2009, was open sourced in 2010, and in 2013 its code was donated to Apache, becoming Apache Spark. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](images/SparkTrends.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The employees of Databricks have written over 75% of the code in Apache Spark and have contributed more than 10 times more code than any other organization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://www.databricks.com/en-website-assets/static/0ba77ee7cfc7bfe140a683b947071484/19830.png)\n",
    "https://www.databricks.com/spark/about"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Apache Spark is a sophisticated distributed computation framework for executing code in parallel across many different machines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://media.licdn.com/dms/image/D4D12AQG1hNjnq0uHdw/article-cover_image-shrink_720_1280/0/1686844447449?e=1719446400&v=beta&t=T1hG-QVH7brUS4xkrkh-mu7ommgnibbgPzfRKqehp24)\n",
    "\n",
    "https://www.linkedin.com/pulse/exploring-world-distributed-computing-frameworks-empowering-nath/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "While the abstractions and interfaces are simple, managing clusters of computers and ensuring production-level stability is not. Databricks makes big data simple by providing Apache Spark as a hosted solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**everyone** sells Spark as a service in Cloud\n",
    "\n",
    "- https://cloud.google.com/solutions/spark\n",
    "- https://aws.amazon.com/it/emr/features/spark/\n",
    "- https://learn.microsoft.com/it-it/azure/hdinsight/spark/apache-spark-overview\n",
    "- https://www.oracle.com/it/big-data/data-flow/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Genesis of Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Hadoop (2004)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Doug Cutting:\n",
    "\n",
    "> The name my kid gave a stuffed yellow elephant. Short, relatively easy to spell and pronounce, meaningless, and not used elsewhere: those are my naming criteria. Kids are good at generating such. Googol is a kid’s term” Hadoop is hardly the first unusual name to be attached to a tech company, of course. Google was born from a misspelling of \"googol\" (1 followed by 100 zeros), which itself was invented when a mathematician was playing with his nephew and together they came up with a name for really big numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://camo.githubusercontent.com/7b15805c76844c10826e2c17f20f644a0aa3201d87083e864441ed84d9681f5f/68747470733a2f2f666d2e636e62632e636f6d2f6170706c69636174696f6e732f636e62632e636f6d2f7265736f75726365732f696d672f656469746f7269616c2f323031332f30352f32332f3130303736323131302d6861646f6f702e353330783239382e6a70673f763d31333639373537303830)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Doug Cutting\n",
    "\n",
    "[Source](https://www.cnbc.com/id/100769719)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://minimalistquotes.com/wp-content/uploads/2022/08/simple-things-should-be-simple-and-complex-things-.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The question then became:\n",
    "there a way to make Hadoop and MR simpler and faster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Development History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Spark’s Early Years at AMPLab (2009) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- First Paper 10-20x faster then map reduce (2010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Spark 1.0 Released (2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Spark 2.0: Unifying DataFrame and Dataset. Structured Streaming (2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Spark 3.0: Hadoop 3.0 support, Support for Pandas, SQL Engine Faster (2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Spark 3.4: [Introducing Spark Connect](https://spark.apache.org/docs/latest/spark-connect-overview.html) (2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Spark 3.5: Spark Connect GA, distributed training with [DeepSpeed](https://www.deepspeed.ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## At the end\n",
    "\n",
    "![](https://i.imgflip.com/7dcyqm.jpg)\n",
    "[NicsMeme](https://imgflip.com/i/7dcyqm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Design Philosophy\n",
    "\n",
    "Spark’s design philosophy centers around four key characteristics:\n",
    "\n",
    "- Speed\n",
    "- Ease of use\n",
    "- Modularity\n",
    "- Extensibility\n",
    "\n",
    "https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch01.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://cc-media-foxit.fichub.com/image/fox-it-mondofox/0177f439-3c0f-44ae-9803-c25f8bfac0dd/flash-vs-superman-game-2jpg-maxw-824.jpg)\n",
    "\n",
    "https://www.reddit.com/r/DCcomics/comments/271ueb/the_definitive_answer_to_flash_vs_superman/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Run workloads 100x faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Logistic Regression](https://spark.apache.org/images/logistic-regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Apache Spark achieves:\n",
    "- high performance for both batch and streaming data\n",
    "- using a state-of-the-art DAG scheduler\n",
    "- a query optimizer\n",
    "- a physical execution engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why Spark is faster ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1. Hardware improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Today’s commodity servers come cheap, with hundreds of gigabytes of memory, multiple cores, and the underlying Unix-based operating system taking advantage of efficient multithreading and parallel processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://5.imimg.com/data5/SELLER/Default/2023/10/353578866/VS/YQ/LQ/200119173/top-quality-trimmed-gold-ram-finger-scrap-5-tons-500x500.jpg)\n",
    "[RAM SCRAP](https://m.indiamart.com/proddetail/top-quality-trimmed-gold-ram-finger-scrap-5-tons-2852693519773.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2. Direct Acyclic Graph (DAG) Scheduler and Query Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Provides an efficient computational graph that can usually be decomposed into tasks that are executed in parallel across workers on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://www.researchgate.net/publication/336769100/figure/fig2/AS:817393752371221@1571893265396/Spark-DAG-for-a-WordCount-application-with-two-stages-each-consisting-of-three-tasks.png)\n",
    "\n",
    "https://www.researchgate.net/publication/336769100_Artificial_neural_networks_based_techniques_for_anomaly_detection_in_Apache_Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ease of Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](http://www.quickmeme.com/img/4d/4d4759d82ce65de86834ff151bc8b419f89f4e2f0d003f10a54b236785e3e6d2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Modularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Write applications quickly in Java, Scala, Python, R, and SQL.\n",
    "\n",
    "Spark offers over 80 high-level operators that make it easy to build parallel apps. \n",
    "\n",
    "And you can use it **interactively** from the Scala, Python, R, and SQL shells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Scala Example\n",
    "\n",
    "\n",
    "```scala\n",
    "df = spark.read.json(\"logs.json\") \n",
    "df.where(\"age > 21\").select(\"name.first\").show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Combine SQL, streaming, and complex analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Spark powers a stack of libraries including:\n",
    "- **Spark SQL** module for working with structured data\n",
    "- **Spark Streaming** build streaming applications and pipelines\n",
    "- **MLlib** scalable machine learning library\n",
    "- **GraphX** API for graphs and graph-parallel computation\n",
    "New:\n",
    "- **Pandas API**: Use pandas syntax on Spark\n",
    "- **Spark Connect**: Client application that communicate with remote Spark server\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://spark.apache.org/images/spark-stack.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You can combine these libraries seamlessly in the same application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Runs everywhere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://images2.corriereobjects.it/methode_image/socialshare/2014/10/07/f143a1aa-4e22-11e4-b38c-5070a4632162.jpg)\n",
    "\n",
    "https://www.corriere.it/foto-gallery/esteri/14_ottobre_07/nuovo-attrezzo-fare-sport-ruota-criceti-misura-d-uomo-809cb22a-4e22-11e4-b38c-5070a4632162.shtml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Spark runs on Hadoop, Apache Mesos, Kubernetes, standalone, or in the cloud. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You can run Spark using its standalone cluster mode, on EC2, on Hadoop YARN, on Mesos, or on Kubernetes\n",
    "![](https://spark.apache.org/images/spark-runs-everywhere.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### It can access diverse external data sources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Analyse\n",
    "Spark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat.\n",
    "\n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html#external-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Query\n",
    "Spark SQL supports operating on a variety of data sources through the DataFrame interface. A DataFrame can be operated on using relational transformations and can also be used to create a temporary view. Registering a DataFrame as a temporary view allows you to run SQL queries over its data.\n",
    "\n",
    "https://spark.apache.org/docs/latest/sql-data-sources.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# In short\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Apache Spark is a fast and general-purpose cluster computing system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark (2024)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Batch/streaming data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Unify the processing of your data in batches and real-time streaming, using your preferred language: Python, SQL, Scala, Java or R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://spark.apache.org/images/batch-sstreaming-data-icon.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## SQL analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Execute fast, distributed ANSI SQL queries for dashboarding and ad-hoc reporting. Runs faster than most data warehouses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://spark.apache.org/images/sql-analytics-icon.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data science at scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Perform Exploratory Data Analysis (EDA) on petabyte-scale data without having to resort to downsampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://spark.apache.org/images/data-science-scale-icon.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Train machine learning algorithms on a laptop and use the same code to scale to fault-tolerant clusters of thousands of machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://spark.apache.org/images/machine-learning-icon.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Running basic example of Spark in docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**nics vanilla**\n",
    "\n",
    "~~Download from https://spark.apache.org/downloads.html into spark/setup\n",
    "We are going to use Spark 3.4.0 Prebuilt for Hadoop 3.3 and later \n",
    "https://dlcdn.apache.org/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz\n",
    "~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**official image for spark**\n",
    "\n",
    "Since 27/06/23\n",
    "\n",
    "https://github.com/apache/spark-docker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SparkPI\n",
    "https://github.com/apache/spark/blob/master/examples/src/main/python/pi.py\n",
    "\n",
    "Use Monte Carlo Method  https://theabbie.github.io/blog/estimate-pi-using-random-numbers.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Run sparkExamplePi.sh\n",
    "```bash\n",
    "docker run -it --rm apache/spark /opt/spark/bin/run-example SparkPi 10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Spark Shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[Spark Shell](https://spark.apache.org/docs/latest/quick-start.html#interactive-analysis-with-the-spark-shell) provides a simple way to learn the API, as well as a powerful tool to analyze data interactively. \n",
    "\n",
    "It is available in either Scala (which runs on the Java VM and is thus a good way to use existing Java libraries) or Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Run a Scala Spark shell in docker\n",
    "```bash\n",
    "# change the path according to your setup, remember should be a fullpath\n",
    "docker run --hostname spark -p 4040 -it --rm -v /home/tap/tap-workspace/tap2024/spark/dataset:/tmp/dataset  apache/spark /opt/spark/bin/spark-shell\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Spark shell in action\n",
    "\n",
    "```\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "24/04/27 14:11:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Spark context Web UI available at http://spark:4040\n",
    "Spark context available as 'sc' (master = local[*], app id = local-1714227106342).\n",
    "Spark session available as 'spark'.\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.1\n",
    "      /_/\n",
    "\n",
    "Using Scala version 2.12.18 (OpenJDK 64-Bit Server VM, Java 11.0.22)\n",
    "Type in expressions to have them evaluated.\n",
    "Type :help for more information.\n",
    "\n",
    "scala>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Execute some commands\n",
    "```scala\n",
    "scala> val textFile=spark.read.textFile(\"file:///tmp/dataset/lotr_characters.csv\");\n",
    "scala> textFile\n",
    "scala> textFile.count();\n",
    "scala> textFile.first();\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Start Py Spark Docker\n",
    "```bash\n",
    "docker run --hostname spark -p 4040:4040 -it --rm -v /home/tap/tap-workspace/tap2024/spark/dataset:/tmp/dataset  apache/spark /opt/spark/bin/pyspark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Create a RDD from a python list\n",
    "```python\n",
    "# Create a list\n",
    "data = range(10000) \n",
    "# Create a RDD using parallelize. \n",
    "distData = sc.parallelize(data) \n",
    "# who is sc ?\n",
    "sc\n",
    "# and distData ?\n",
    "distData\n",
    "# Let's list\n",
    "distData.collect()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Create a RDD from a text file\n",
    "```python\n",
    "# An RDD can be also created from external storage\n",
    "# textFile creates a RDD(String) (remember when we use spark.read.file)\n",
    "distFile = sc.textFile(\"/tmp/dataset/The Return Of The King_djvu.txt\") \n",
    "distFile\n",
    "\n",
    "# Take the first ones\n",
    "distFile.take(10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A simple map-reduce function\n",
    "```python\n",
    "sizeOfBook=distFile.map(lambda s: len(s)).reduce(lambda a, b: a + b)\n",
    "sizeOfBook\n",
    "\n",
    "# Let's do in steps\n",
    "\n",
    "# First step for each line compute the lenght of the line \n",
    "mappa=distFile.map(lambda s: len(s))\n",
    "# Show some elements\n",
    "mappa.take(20)\n",
    "# Then sum all the elements of the RDD \n",
    "reduce=mappa.reduce(lambda a, b: a + b)\n",
    "reduce\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Key-Value Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Typically Spark operations work on RDDs containing any type of objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Few operations are only available on RDDs of key-value pairs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The most common ones are distributed “shuffle” operations, such as grouping or aggregating the elements by a key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In Python, these operations work on RDDs containing built-in Python [tuples](https://realpython.com/python-tuple/) such as (1, 2). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In pyspark create RDD of tuples and then call your desired operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A line count example\n",
    "```python\n",
    "# Create a RDD of pairs from file, \n",
    "pairs = distFile.map(lambda s: (s, 1))\n",
    "pairs\n",
    "# We have create a new RDD, let's see what it contains\n",
    "pairs.take(50)\n",
    "# Now we can use a reduce function, to count how may times the line appears in the document\n",
    "counts = pairs.reduceByKey(lambda a, b: a + b)\n",
    "counts.take(50)\n",
    "# Let's order by key\n",
    "ordered=counts.sortByKey()\n",
    "# and take ordered\n",
    "ordered.takeOrdered(10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's do a better analysis\n",
    "Which is the most frequent word in the book ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Most frequent word in the book\n",
    "```python\n",
    "words=distFile.flatMap(lambda line:line.split(\" \"))\n",
    "words.take(100)\n",
    "# Great, let's assign a counter and then sum\n",
    "wordCounters=words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "wordCounters.take(10)\n",
    "# Ok I want to sort now\n",
    "wordsSorted=wordCounters.takeOrdered(200, key = lambda x: -x[1])\n",
    "wordsSorted\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": "true",
   "footer": "<div class=\"tap-footer\"> *** Technologies for advanced programming (TAP) - 2024 ***</div>",
   "header": "<div class=\"tap-header\"></div>",
   "scroll": true,
   "theme": "white"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
