{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Kafka Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Kafka Streams is a client library for building mission-critical real-time applications and microservices, where the input and/or output data is stored in Kafka clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Kafka Streams combines the simplicity of writing and deploying standard Java and Scala applications on the client side with the benefits of Kafka's server-side cluster technology to make these applications highly scalable, elastic, fault-tolerant, distributed, and much more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Highlights \n",
    "https://kafka.apache.org/35/documentation/streams/core-concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Designed as a **simple and lightweight client library**, which can be easily embedded in any Java application and integrated with any existing packaging, deployment and operational tools that users have for their streaming applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "i.e a dependency in a \n",
    "```xml\n",
    "<dependency>\n",
    "    <groupId>org.apache.kafka</groupId>\n",
    "    <artifactId>kafka-streams</artifactId>\n",
    "    <version>3.5.0</version>\n",
    "</dependency>\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Has **no external dependencies on systems other than Apache Kafka itself** as the internal messaging layer; notably, it uses Kafka's partitioning model to horizontally scale processing while maintaining strong ordering guarantees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://media.makeameme.org/created/keep-it-l0ywkb.jpg)\n",
    "[Source](https://makeameme.org/meme/keep-it-l0ywkb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Supports **fault-tolerant local state**, which enables very fast and efficient stateful operations like windowed joins and aggregations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://kafka.apache.org/0102/images/streams-architecture-states.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Supports **exactly-once processing** semantics to guarantee that each record will be processed once and only once even when there is a failure on either Streams clients or Kafka brokers in the middle of processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://cdn.confluent.io/wp-content/uploads/kafka-topic.png)\n",
    "\n",
    "https://www.confluent.io/blog/enabling-exactly-once-kafka-streams/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Employs **one-record-at-a-time processing** to achieve millisecond processing latency, and supports **event-time based windowing operations** with out-of-order arrival of records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://images.ctfassets.net/gt6dp23g0g38/y3dPJWV6inVi0KIWNk5Ic/5952ed5d7048e099a12ed57df173a39a/late-record-1.png)\n",
    "\n",
    "https://developer.confluent.io/learn-kafka/kafka-streams/time-concepts/\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Offers necessary stream processing primitives, along with a **high-level Streams DSL** and a **low-level Processor API**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- High Level : https://kafka.apache.org/34/documentation/streams/developer-guide/dsl-api.html\n",
    "- Processor API https://kafka.apache.org/34/documentation/streams/developer-guide/processor-api.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The New York Times\n",
    "\n",
    "![](https://kafka.apache.org/images/powered-by/NYT.jpg)\n",
    "\n",
    "The New York Times uses Apache Kafka and the Kafka Streams to store and distribute, in real-time, published content to the various applications and systems that make it available to the readers.\n",
    "\n",
    "[https://open.nytimes.com/publishing-with-apache-kafka-at-the-new-york-times-7f0e3b7d2077]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Real Time Analytics\n",
    "\n",
    "![](https://dzone.com/storage/temp/12275703-kafka-use-case.png)\n",
    "\n",
    "Story: https://dzone.com/articles/real-time-stream-processing-with-apache-kafka-part-1\n",
    "\n",
    "Code: https://github.com/hellosatish/microservice-patterns/tree/master/vehicle-tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Wordcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Run Demo App\n",
    "``` bash\n",
    "# Start Zk\n",
    "docker run -e KAFKA_ACTION=start-zk --network tap --ip 10.0.100.22  -p 2181:2181 --name kafkaZK -it tap:kafka\n",
    "# Start Kafka Server\n",
    "docker run -e KAFKA_ACTION=start-kafka --network tap --ip 10.0.100.23  -p 9092:9092 --name kafkaServer -it tap:kafka\n",
    "\n",
    "# Create Topics (need to be created before start the stream)\n",
    "docker exec -it kafkaServer kafka-topics.sh --bootstrap-server kafkaServer:9092 --create --topic streams-plaintext-input\n",
    "\n",
    "docker exec -it kafkaServer kafka-topics.sh --bootstrap-server kafkaServer:9092 --create --topic streams-wordcount-output\n",
    "\n",
    "# kafkaWordCountStream\n",
    "docker exec -it kafkaServer /opt/kafka/bin/kafka-run-class.sh org.apache.kafka.streams.examples.wordcount.WordCountDemo\n",
    "\n",
    "# Start a producer \n",
    "docker run --rm -e KAFKA_ACTION=producer -e KAFKA_TOPIC=streams-plaintext-input --network tap  -it tap:kafka\n",
    "\n",
    "# Start consumer\n",
    "docker run --rm -e KAFKA_ACTION=consumer -e KAFKA_TOPIC=streams-wordcount-output -e KAFKA_CONSUMER_PROPERTIES=\"--formatter kafka.tools.DefaultMessageFormatter --property print.key=true --property print.value=true --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer\" --network tap   -it tap:kafka\n",
    "```\n",
    "\n",
    "Messages:\n",
    "- all streams lead to kafka\n",
    "- hello kafka streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Behind the scenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://kafka.apache.org/34/images/streams-table-updates-01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://kafka.apache.org/34/images/streams-table-updates-02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stream Processing Topology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- A stream is the most important abstraction provided by Kafka Streams: it represents an unbounded, continuously updating data set. A stream is an ordered, replayable, and fault-tolerant sequence of immutable data records, where a data record is defined as a key-value pair.\n",
    "- A stream processing application is any program that makes use of the Kafka Streams library. It defines its computational logic through one or more processor topologies, where a processor topology is a graph of stream processors (nodes) that are connected by streams (edges).\n",
    "- A stream processor is a node in the processor topology; it represents a processing step to transform data in streams by receiving one input record at a time from its upstream processors in the topology, applying its operation to it, and may subsequently produce one or more output records to its downstream processors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://kafka.apache.org/34/images/streams-architecture-topology.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Kafka Streams offers two ways to define the stream processing topology "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "the Kafka Streams DSL provides the most common data transformation operations such as map, filter, join and aggregations out of the box; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Processor API allows developers define and connect custom processors as well as to interact with state stores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Duality of Streams and Tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When implementing stream processing use cases in practice, you typically need both streams and also databases. An example use case that is very common in practice is an e-commerce application that enriches an incoming stream of customer transactions with the latest customer information from a database table. In other words, streams are everywhere, but databases are everywhere, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Stream as Table**: A stream can be considered a changelog of a table, where each data record in the stream captures a state change of the table. A stream is thus a table in disguise, and it can be easily turned into a \"real\" table by replaying the changelog from beginning to end to reconstruct the table. Similarly, in a more general analogy, aggregating data records in a stream - such as computing the total number of pageviews by user from a stream of pageview events - will return a table (here with the key and the value being the user and its corresponding pageview count, respectively).b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Table as Stream**: A table can be considered a snapshot, at a point in time, of the latest value for each key in a stream (a stream's data records are key-value pairs). A table is thus a stream in disguise, and it can be easily turned into a \"real\" stream by iterating over each key-value entry in the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```java\n",
    "// Serializers/deserializers (serde) for String and Long types\n",
    "final Serde<String> stringSerde = Serdes.String();\n",
    "final Serde<Long> longSerde = Serdes.Long();\n",
    " \n",
    "// Construct a `KStream` from the input topic \"streams-plaintext-input\", where message values\n",
    "// represent lines of text (for the sake of this example, we ignore whatever may be stored\n",
    "// in the message keys).\n",
    "KStream<String, String> textLines = builder.stream(\n",
    "      \"streams-plaintext-input\",\n",
    "      Consumed.with(stringSerde, stringSerde)\n",
    "    );\n",
    " \n",
    "KTable<String, Long> wordCounts = textLines\n",
    "    // Split each text line, by whitespace, into words.\n",
    "    .flatMapValues(value -> Arrays.asList(value.toLowerCase().split(\"\\\\W+\")))\n",
    " \n",
    "    // Group the text words as message keys\n",
    "    .groupBy((key, value) -> value)\n",
    " \n",
    "    // Count the occurrences of each word (message key).\n",
    "    .count();\n",
    " \n",
    "// Store the running counts as a changelog stream to the output topic.\n",
    "wordCounts.toStream().to(\"streams-wordcount-output\", Produced.with(Serdes.String(), Serdes.Long()));\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stream and Tables: A Primer\n",
    "https://www.confluent.io/blog/kafka-streams-tables-part-1-event-streaming/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Event Record\n",
    "\n",
    "**An event records the fact that “something happened” in the world**\n",
    "\n",
    "- Event key: “Alice”\n",
    "- Event value: “Has arrived in Rome”\n",
    "- Event timestamp: “Dec. 3, 2019 at 9:06 a.m.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Event Stream\n",
    "\n",
    "**An event stream records the history of what has happened in the world as a sequence of events**\n",
    "\n",
    "This history is an ordered sequence or chain of events, so we know which event happened before another event to infer causality.\n",
    "\n",
    "A stream thus represents both the past and the present: as we go from today to tomorrow—or from one millisecond to the next—new events are constantly being appended to the history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "_The sequence of moves in a chess match_\n",
    "\n",
    "White moved the e2 pawn to e4, then Black moved the e7 pawn to e5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](https://66.media.tumblr.com/tumblr_m8ok25dsch1r8gmlso1_500.gifv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Event Table\n",
    "\n",
    "**A table represents the state of the world** at a particular point in time, typically “now.”\n",
    "\n",
    "![](https://cdn.confluent.io/wp-content/uploads/streams-vs-tables-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| Stream | Table |\n",
    "| ------ | ----- |\n",
    "|A stream provides immutable data. It supports only inserting (appending) new events, whereas existing events cannot be changed. Streams are persistent, durable, and fault tolerant. Events in a stream can be keyed, and you can have many events for one key, like “all of Bob’s payments.” If you squint a bit, you could consider a stream to be like a table in a relational database (RDBMS) that has no unique key constraint and that is append only.| A table provides mutable data. New events—rows—can be inserted, and existing rows can be updated and deleted. Here, an event’s key aka row key identifies which row is being mutated. Like streams, tables are persistent, durable, and fault tolerant. Today, a table behaves much like an RDBMS materialized view because it is being changed automatically as soon as any of its input streams or tables change, rather than letting you directly run insert, update, or delete operations against it.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "|                                           | Stream |  Table    |\n",
    "|-------------------------------------------|--------|-----------|\n",
    "| First event with key bob arrives          | Insert | Insert    |\n",
    "| Another event with key bob arrives        | Insert | Update    |\n",
    "| Event with key bob and value null arrives | Insert | Delete    |\n",
    "| Event with key null arrives               | Insert | _ignored_ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://cdn.confluent.io/wp-content/uploads/event-stream-1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Writing App\n",
    "https://docs.confluent.io/platform/current/streams/developer-guide/running-app.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```bash\n",
    "mvn archetype:generate \\\n",
    "    -DarchetypeGroupId=org.apache.kafka \\\n",
    "    -DarchetypeArtifactId=streams-quickstart-java \\\n",
    "    -DarchetypeVersion=3.5.0 \\\n",
    "    -DgroupId=streams.examples \\\n",
    "    -DartifactId=kafka-streams.examples \\\n",
    "    -Dversion=0.1 \\\n",
    "    -Dpackage=tap\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Let's make it run in Docker\n",
    "\n",
    "- change props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\"); into kafkaServer:9092\n",
    "- create topics\n",
    "- build tap:kafkastream using build.sh\n",
    "- run using docker run --network tap --rm -it tap:kafkastream class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pipe\n",
    "``` bash\n",
    "# Start Zk\n",
    "docker run -e KAFKA_ACTION=start-zk --network tap --ip 10.0.100.22  -p 2181:2181 --name kafkaZK -it tap:kafka\n",
    "# Start Kafka Server\n",
    "docker run -e KAFKA_ACTION=start-kafka --network tap --ip 10.0.100.23  -p 9092:9092 --name kafkaServer -it tap:kafka\n",
    "\n",
    "# Create Topics (need to be created before start the stream)\n",
    "docker exec -it kafkaServer kafka-topics.sh --bootstrap-server kafkaServer:9092 --create --topic streams-plaintext-input\n",
    "docker exec -it kafkaServer kafka-topics.sh --bootstrap-server kafkaServer:9092 --create --topic streams-pipe-output\n",
    "\n",
    "# Run \n",
    "docker run --network tap --rm -it tap:kafkastream tap.Pipe\n",
    "# Start a producer \n",
    "docker run --rm -e KAFKA_ACTION=producer -e KAFKA_TOPIC=streams-plaintext-input --network tap  -it tap:kafka\n",
    "\n",
    "# Start consumer\n",
    "docker run --rm -e KAFKA_ACTION=consumer -e KAFKA_TOPIC=streams-pipe-output --network tap -it tap:kafka\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### LineSplit\n",
    "```bash\n",
    "# Start Zk\n",
    "docker run -e KAFKA_ACTION=start-zk --network tap --ip 10.0.100.22  -p 2181:2181 --name kafkaZK -it tap:kafka\n",
    "# Start Kafka Server\n",
    "docker run -e KAFKA_ACTION=start-kafka --network tap --ip 10.0.100.23  -p 9092:9092 --name kafkaServer -it tap:kafka\n",
    "\n",
    "# Create Topics (need to be created before start the stream)\n",
    "docker exec -it kafkaServer kafka-topics.sh --bootstrap-server kafkaServer:9092 --create --topic streams-plaintext-input\n",
    "docker exec -it kafkaServer kafka-topics.sh --bootstrap-server kafkaServer:9092 --create --topic streams-linesplit-output\n",
    "\n",
    "# Run \n",
    "docker run --network tap --rm -it tap:kafkastream tap.LineSplit\n",
    "# Start a producer \n",
    "docker run --rm -e KAFKA_ACTION=producer -e KAFKA_TOPIC=streams-plaintext-input --network tap  -it tap:kafka\n",
    "\n",
    "# Start consumer\n",
    "docker run --rm -e KAFKA_ACTION=consumer -e KAFKA_TOPIC=streams-linesplit-output --network tap -it tap:kafka\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Biblio\n",
    "- https://blog.softwaremill.com/do-not-reinvent-the-wheel-use-kafka-connect-4bcabb143292\n",
    "- https://dev.to/thegroo/kafka-connect-crash-course-1chd\n",
    "- https://data-flair.training/blogs/kafka-connect/\n",
    "- https://www.confluent.io/blog/simplest-useful-kafka-connect-data-pipeline-world-thereabouts-part-1/"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "rise": {
   "autolaunch": true,
   "enable_chalkboard": "true",
   "footer": "<div class=\"tap-footer\"> *** Technologies for advanced programming (TAP) - 2023 ***</div>",
   "header": "<div class=\"tap-header\"></div>",
   "scroll": true,
   "theme": "white"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
